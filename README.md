# ğŸ•·ï¸ Web Crawler Demo - Email Extractor

Un web crawler professionnel pour extraire les adresses emails (liens `mailto:`) d'une page web et de ses pages rÃ©fÃ©rencÃ©es, avec contrÃ´le de la profondeur, rate limiting, respect des robots.txt et logging structurÃ©.

## ğŸ“‹ Description

Ce projet implÃ©mente un web crawler de niveau production qui :
- âœ… Extrait tous les emails distincts trouvÃ©s dans les liens `mailto:` d'une page HTML
- âœ… Explore rÃ©cursivement les pages web rÃ©fÃ©rencÃ©es
- âœ… Utilise un algorithme **BFS (Breadth-First Search)** pour prioriser les pages les plus proches
- âœ… Parse du **HTML rÃ©el malformÃ©** avec HtmlAgilityPack
- âœ… Supporte les **URLs HTTP/HTTPS** et domaines multiples
- âœ… ImplÃ©mente le **rate limiting** et politiques de politesse
- âœ… Respecte les fichiers **robots.txt**
- âœ… Logging structurÃ© avec **Serilog**
- âœ… Permet de contrÃ´ler la profondeur maximale de recherche
- âœ… GÃ¨re les cycles (pages qui se rÃ©fÃ©rencent mutuellement)
- âœ… Garantit l'unicitÃ© des emails retournÃ©s
- âœ… **23 tests unitaires xUnit** (100% de rÃ©ussite)

## ğŸ¯ Algorithme : BFS (Breadth-First Search)

### Pourquoi BFS ?

L'algorithme **BFS** a Ã©tÃ© choisi car il :
- âœ… Explore tous les liens Ã  profondeur `n` avant de passer Ã  profondeur `n+1`
- âœ… Respecte parfaitement la contrainte de prioritÃ© aux pages "les plus proches"
- âœ… Ã‰vite d'aller trop loin en profondeur avant d'avoir explorÃ© le niveau actuel
- âœ… Garantit une exploration systÃ©matique et contrÃ´lÃ©e

### Comparaison avec DFS

| CritÃ¨re | BFS | DFS |
|---------|-----|-----|
| Ordre d'exploration | Niveau par niveau | Branche par branche |
| Pages proches | âœ… Prioritaires | âŒ Pas garanties |
| ContrÃ´le profondeur | âœ… Naturel | âš ï¸ Plus complexe |
| MÃ©moire | Plus Ã©levÃ©e | Plus faible |

## ğŸš€ Utilisation

### Exemple de Base (MockWebBrowser)

```csharp
// Configuration du logging
LoggingConfiguration.ConfigureLogging(LogEventLevel.Information);

var browser = new MockWebBrowser();
var webCrawler = new EmailWebCrawler();

// Profondeur 0 : page de dÃ©part uniquement
var emails = webCrawler.GetEmailsInPageAndChildPages(browser, "C:/TestHtml/index.html", 0);
// RÃ©sultat : ["nullepart@mozilla.org"]

// Profondeur 1 : page de dÃ©part + pages directement rÃ©fÃ©rencÃ©es
emails = webCrawler.GetEmailsInPageAndChildPages(browser, "C:/TestHtml/index.html", 1);
// RÃ©sultat : ["nullepart@mozilla.org", "ailleurs@mozilla.org"]

// Profondeur 2 : page de dÃ©part + 2 niveaux de pages rÃ©fÃ©rencÃ©es
emails = webCrawler.GetEmailsInPageAndChildPages(browser, "C:/TestHtml/index.html", 2);
// RÃ©sultat : ["nullepart@mozilla.org", "ailleurs@mozilla.org", "loin@mozilla.org"]
```

### Exemple avec HTTP et Rate Limiting

```csharp
// Politique Conservative : crawling respectueux
var policies = CrawlerPolicies.Conservative;
var httpBrowser = new HttpWebBrowser(policies.UserAgent, policies.DelayBetweenRequestsMs);
var crawler = new EmailWebCrawler(policies);

var emails = crawler.GetEmailsInPageAndChildPages(
    httpBrowser, 
    "https://example.com", 
    maximumDepth: 2
);

// Fermer proprement le logger
LoggingConfiguration.CloseAndFlush();
```

### Politiques de Crawling

```csharp
// Politique par dÃ©faut (Ã©quilibrÃ©e)
var defaultPolicies = CrawlerPolicies.Default;
// - DÃ©lai: 1000ms entre requÃªtes
// - Max pages: illimitÃ©
// - Respecte robots.txt

// Politique conservative (trÃ¨s respectueuse)
var conservative = CrawlerPolicies.Conservative;
// - DÃ©lai: 2000ms entre requÃªtes
// - Max pages: 100 par domaine
// - Respecte robots.txt

// Politique aggressive (rapide, tests uniquement)
var aggressive = CrawlerPolicies.Aggressive;
// - DÃ©lai: 100ms entre requÃªtes
// - Max pages: illimitÃ©
// - Ignore robots.txt

// Politique personnalisÃ©e
var customPolicies = new CrawlerPolicies
{
    DelayBetweenRequestsMs = 500,
    MaxPagesPerDomain = 50,
    RequestTimeoutSeconds = 15,
    UserAgent = "MonCrawler/1.0",
    RespectRobotsTxt = true
};
```

## ğŸ“ Structure du Projet

```
WebCrawlerDemo/
â”œâ”€â”€ Interfaces.cs              # ITheTest et IWebBrowser
â”œâ”€â”€ EmailWebCrawler.cs         # Algorithme BFS avec robots.txt
â”œâ”€â”€ MockWebBrowser.cs          # Navigateur fictif pour les tests
â”œâ”€â”€ HttpWebBrowser.cs          # Navigateur HTTP avec rate limiting
â”œâ”€â”€ CrawlerPolicies.cs         # Politiques de crawling configurables
â”œâ”€â”€ RobotsTxtParser.cs         # Parser RFC-compliant pour robots.txt
â”œâ”€â”€ RobotsTxtCache.cs          # Cache thread-safe pour robots.txt
â”œâ”€â”€ LoggingConfiguration.cs    # Configuration Serilog centralisÃ©e
â”œâ”€â”€ Program.cs                 # DÃ©monstration de toutes les itÃ©rations
â”œâ”€â”€ WebCrawlerDemo.Tests/      # 23 tests unitaires xUnit
â”‚   â”œâ”€â”€ EmailWebCrawlerTests.cs
â”‚   â”œâ”€â”€ RobotsTxtParserTests.cs
â”‚   â””â”€â”€ CrawlerPoliciesTests.cs
â”œâ”€â”€ logs/                      # Logs Serilog (rotation quotidienne)
â””â”€â”€ README.md                  # Cette documentation
```

## ğŸ”§ Technologies

- **.NET 8.0**
- **C# 12** avec nullable reference types
- **HtmlAgilityPack 1.12.4** - Parsing HTML robuste (supporte HTML malformÃ©)
- **Serilog 4.3.0** - Logging structurÃ©
  - Serilog.Sinks.Console 6.0.0
  - Serilog.Sinks.File 6.0.0
- **xUnit 2.9.3** - Framework de tests unitaires
- **SOLID Principles** - Architecture maintenable et extensible

## ğŸ“Š Exemple de Test

### Structure HTML de Test

**index.html**
```html
<html>
  <h1>INDEX</h1>
  <a href="./child1.html">child1</a>
  <a href="mailto:nullepart@mozilla.org">Envoyer l'email nulle part</a>
</html>
```

**child1.html**
```html
<html>
  <h1>CHILD1</h1>
  <a href="./index.html">index</a>
  <a href="./child2.html">child2</a>
  <a href="mailto:ailleurs@mozilla.org">Envoyer l'email ailleurs</a>
  <a href="mailto:nullepart@mozilla.org">Envoyer l'email nulle part</a>
</html>
```

**child2.html**
```html
<html>
  <h1>CHILD2</h1>
  <a href="./index.html">index</a>
  <a href="mailto:loin@mozilla.org">Envoyer l'email loin</a>
  <a href="mailto:nullepart@mozilla.org">Envoyer l'email nulle part</a>
</html>
```

### RÃ©sultats Attendus

```
Profondeur 0: nullepart@mozilla.org
Profondeur 1: nullepart@mozilla.org, ailleurs@mozilla.org
Profondeur 2: nullepart@mozilla.org, ailleurs@mozilla.org, loin@mozilla.org
```

## âš¡ CaractÃ©ristiques Techniques

### Gestion des Cycles
Le crawler dÃ©tecte et Ã©vite les cycles grÃ¢ce Ã  un `HashSet<string>` qui stocke les URLs dÃ©jÃ  visitÃ©es.

```csharp
var visitedUrls = new HashSet<string>();
if (visitedUrls.Contains(currentUrl))
    continue;
```

### Extraction des Emails
Les emails sont extraits des liens `mailto:` et validÃ©s avec une expression rÃ©guliÃ¨re :

```csharp
var mailtoLinks = doc.Descendants("a")
    .Where(a => a.Attribute("href")?.Value.StartsWith("mailto:") == true);
```

### RÃ©solution des URLs Relatives
Le crawler gÃ¨re correctement les URLs relatives (`./child1.html`) :

```csharp
if (relativeUrl.StartsWith("./"))
    relativeUrl = relativeUrl.Substring(2);
string absoluteUrl = Path.Combine(baseDirectory, relativeUrl);
```

### UnicitÃ© des RÃ©sultats
Un `HashSet<string>` garantit qu'aucun email n'est retournÃ© en double, mÃªme s'il apparaÃ®t sur plusieurs pages.

## ğŸ“ˆ ComplexitÃ©

- **Temps** : O(V + E)
  - V = nombre de pages visitÃ©es
  - E = nombre de liens explorÃ©s
  
- **Espace** : O(V)
  - Stockage des URLs visitÃ©es
  - Queue pour le parcours BFS

## ğŸƒ ExÃ©cution

```bash
# Cloner le repository
git clone https://github.com/[username]/WebCrawlerDemo.git

# Naviguer dans le projet
cd WebCrawlerDemo

# Restaurer les dÃ©pendances
dotnet restore

# ExÃ©cuter l'application
dotnet run
```

## ğŸ§ª Tests

### Tests Unitaires (xUnit)

**23 tests unitaires avec 100% de rÃ©ussite** couvrant :

#### EmailWebCrawlerTests (10 tests)
- âœ… Extraction correcte des emails Ã  diffÃ©rentes profondeurs (0, 1, 2)
- âœ… Gestion du HTML malformÃ© avec HtmlAgilityPack
- âœ… Gestion des emails en double (unicitÃ©)
- âœ… Normalisation des emails en minuscules
- âœ… Respect de la limite de pages par domaine
- âœ… Gestion des emails distincts sur plusieurs pages
- âœ… Validation des paramÃ¨tres (null, empty)

#### RobotsTxtParserTests (10 tests)
- âœ… Parsing de robots.txt vide (tout autorisÃ©)
- âœ… Directive Disallow: / (tout bloquÃ©)
- âœ… Directive Disallow avec chemins spÃ©cifiques
- âœ… Combinaison Allow/Disallow (rÃ¨gle la plus spÃ©cifique gagne)
- âœ… Support Crawl-delay
- âœ… Support Sitemap
- âœ… Gestion des commentaires (ligne complÃ¨te et inline)
- âœ… Patterns avec wildcards (* et $)
- âœ… Multi-user-agents (rÃ¨gles spÃ©cifiques prioritaires)

#### CrawlerPoliciesTests (3 tests)
- âœ… Politique Default (valeurs Ã©quilibrÃ©es)
- âœ… Politique Conservative (respectueuse)
- âœ… Politique Aggressive (tests uniquement)

### ExÃ©cution des Tests

```bash
# ExÃ©cuter tous les tests
dotnet test

# Avec output dÃ©taillÃ©
dotnet test --logger "console;verbosity=detailed"

# Avec coverage (si configurÃ©)
dotnet test --collect:"XPlat Code Coverage"
```

### RÃ©sultat des Tests

```
SÃ©rie de tests rÃ©ussie.
Nombre total de tests : 23
     RÃ©ussi(s) : 23
 DurÃ©e totale : 0,49 secondes
```

## ğŸ“ Principes de Design

### SOLID Principles

- **Single Responsibility** : Chaque mÃ©thode a une responsabilitÃ© unique
- **Open/Closed** : Extensible via l'interface `IWebBrowser`
- **Liskov Substitution** : `MockWebBrowser` peut Ãªtre remplacÃ© par toute implÃ©mentation de `IWebBrowser`
- **Interface Segregation** : Interfaces ciblÃ©es et minimales
- **Dependency Inversion** : Le crawler dÃ©pend de l'abstraction `IWebBrowser`, pas d'une implÃ©mentation concrÃ¨te

### Clean Code

- Nommage expressif des variables et mÃ©thodes
- MÃ©thodes courtes et focalisÃ©es
- Documentation XML pour toutes les mÃ©thodes publiques
- Gestion appropriÃ©e des erreurs

## ğŸ“ Notes Techniques

- Le HTML est traitÃ© comme du XML valide (hypothÃ¨se de l'exercice)
- Les emails sont normalisÃ©s en minuscules pour l'unicitÃ©
- Les URLs absolues HTTP/HTTPS sont ignorÃ©es (focus sur les liens locaux)
- Les paramÃ¨tres dans les liens `mailto:` (ex: `?subject=...`) sont automatiquement retirÃ©s

## ğŸ¤ Contribution

Ce projet est un exercice algorithmique dÃ©montrant l'implÃ©mentation d'un web crawler avec BFS. Les contributions sont bienvenues pour :
- AmÃ©liorer la robustesse du parsing HTML
- Ajouter des tests unitaires (xUnit)
- Optimiser les performances
- Ã‰tendre les fonctionnalitÃ©s

## ğŸ“„ Licence

Ce projet est fourni Ã  des fins Ã©ducatives et de dÃ©monstration.

## ğŸ‘¨â€ğŸ’» Auteur

DÃ©veloppÃ© comme dÃ©monstration d'algorithme de web crawling avec contrÃ´le de profondeur.

## ğŸ“Š ItÃ©rations du Projet

Ce projet a Ã©tÃ© dÃ©veloppÃ© en **7 itÃ©rations** progressives :

### ItÃ©ration 1 : BFS de base
- âœ… Algorithme BFS pour exploration niveau par niveau
- âœ… Parsing XML avec System.Xml.Linq
- âœ… Gestion des cycles et profondeur

### ItÃ©ration 2 : HTML rÃ©el avec HtmlAgilityPack
- âœ… Support du HTML malformÃ©
- âœ… Parser robuste (balises non fermÃ©es, attributs sans guillemets)
- âœ… XPath pour extraction des liens

### ItÃ©ration 3 : URLs HTTP/HTTPS
- âœ… HttpClient pour requÃªtes rÃ©elles
- âœ… RÃ©solution d'URLs relatives avec Uri
- âœ… Normalisation avancÃ©e des URLs
- âœ… Support domaines multiples

### ItÃ©ration 4 : Rate Limiting et Politiques
- âœ… DÃ©lai configurable entre requÃªtes par domaine
- âœ… Limitation de pages par domaine
- âœ… 3 politiques prÃ©configurÃ©es (Default/Conservative/Aggressive)
- âœ… User-Agent personnalisable

### ItÃ©ration 5 : Support robots.txt
- âœ… Parser RFC-compliant (Allow/Disallow/Crawl-delay/Sitemap)
- âœ… Cache thread-safe par domaine (24h)
- âœ… Gestion multi-user-agents
- âœ… Patterns avec wildcards

### ItÃ©ration 6 : Tests Unitaires
- âœ… 23 tests xUnit avec 100% de rÃ©ussite
- âœ… Arrange-Act-Assert pattern
- âœ… Tests de tous les composants principaux
- âœ… Validation des edge cases

### ItÃ©ration 7 : Structured Logging
- âœ… Serilog avec Console et File sinks
- âœ… Logs structurÃ©s JSON dans fichiers
- âœ… 4 niveaux (Debug/Info/Warning/Error)
- âœ… Rotation quotidienne, rÃ©tention 7 jours
- âœ… TraÃ§abilitÃ© complÃ¨te de toutes les opÃ©rations

## ğŸ“ˆ FonctionnalitÃ©s de Production

Ce crawler est **prÃªt pour la production** avec :

- ğŸ›¡ï¸ **SÃ©curitÃ©** : Respect des robots.txt, rate limiting, timeouts
- ğŸ” **ObservabilitÃ©** : Logging structurÃ© avec Serilog
- âœ… **QualitÃ©** : 23 tests unitaires, SOLID principles
- âš¡ **Performance** : Cache robots.txt, normalisation URLs
- ğŸ¯ **Robustesse** : Gestion des erreurs, HTML malformÃ©
- ğŸ”§ **Configuration** : Politiques personnalisables

## ğŸ“ Logging

Les logs sont Ã©crits dans `logs/webcrawler-YYYY-MM-DD.log` avec :

```
[2025-10-29 21:10:07.514 +01:00 INF] DÃ©but du crawling - URL: "https://example.com"
[2025-10-29 21:10:07.607 +01:00 DBG] Cache robots.txt initialisÃ©
[2025-10-29 21:10:07.626 +01:00 DBG] Emails trouvÃ©s - Nombre: 3, Emails: ["a@test.com", "b@test.com"]
[2025-10-29 21:10:07.629 +01:00 INF] Crawling terminÃ© - Emails: 3, Pages: 5
```

---

## ğŸ“ DÃ©veloppement AugmentÃ© par l'IA

Ce projet a Ã©tÃ© dÃ©veloppÃ© en utilisant **GitHub Copilot** comme assistant de dÃ©veloppement senior, dÃ©montrant :
- ğŸš€ DÃ©veloppement itÃ©ratif rapide (7 itÃ©rations complÃ¨tes)
- âœ… Code de qualitÃ© production avec tests
- ğŸ“š Documentation complÃ¨te (README + Wiki)
- ğŸ—ï¸ Architecture SOLID et Clean Code
- ğŸ”„ IntÃ©gration continue (Git + GitHub)

**Voir le Wiki** pour plus de dÃ©tails sur la mÃ©thodologie de dÃ©veloppement augmentÃ© par l'IA.

Test : dotnet run
sortie:
```
[21:10:07 INF] === DÃ©marrage de WebCrawlerDemo ===

=== Test du Web Crawler d'Emails ===

Profondeur 0:
[21:10:07 INF] DÃ©but du crawling - URL: C:/TestHtml/index.html, Profondeur max: 0
[21:10:07 DBG] Emails trouvÃ©s - URL: C:/TestHtml/index.html, Nombre: 1
[21:10:07 INF] Crawling terminÃ© - Emails trouvÃ©s: 1, Pages visitÃ©es: 1
RÃ©sultat: nullepart@mozilla.org
```
