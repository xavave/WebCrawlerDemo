# ğŸ•·ï¸ Web Crawler Demo - Email Extractor

Un algorithme de web crawling pour extraire les adresses emails (liens `mailto:`) d'une page web et de ses pages rÃ©fÃ©rencÃ©es, avec contrÃ´le de la profondeur de recherche.

## ğŸ“‹ Description

Ce projet implÃ©mente un web crawler qui :
- Extrait tous les emails distincts trouvÃ©s dans les liens `mailto:` d'une page HTML
- Explore rÃ©cursivement les pages web rÃ©fÃ©rencÃ©es
- Utilise un algorithme **BFS (Breadth-First Search)** pour prioriser les pages les plus proches
- Permet de contrÃ´ler la profondeur maximale de recherche
- GÃ¨re les cycles (pages qui se rÃ©fÃ©rencent mutuellement)
- Garantit l'unicitÃ© des emails retournÃ©s

## ğŸ¯ Algorithme : BFS (Breadth-First Search)

### Pourquoi BFS ?

L'algorithme **BFS** a Ã©tÃ© choisi car il :
- âœ… Explore tous les liens Ã  profondeur `n` avant de passer Ã  profondeur `n+1`
- âœ… Respecte parfaitement la contrainte de prioritÃ© aux pages "les plus proches"
- âœ… Ã‰vite d'aller trop loin en profondeur avant d'avoir explorÃ© le niveau actuel
- âœ… Garantit une exploration systÃ©matique et contrÃ´lÃ©e

### Comparaison avec DFS

| CritÃ¨re | BFS | DFS |
|---------|-----|-----|
| Ordre d'exploration | Niveau par niveau | Branche par branche |
| Pages proches | âœ… Prioritaires | âŒ Pas garanties |
| ContrÃ´le profondeur | âœ… Naturel | âš ï¸ Plus complexe |
| MÃ©moire | Plus Ã©levÃ©e | Plus faible |

## ğŸš€ Utilisation

```csharp
var browser = new MockWebBrowser();
var webCrawler = new EmailWebCrawler();

// Profondeur 0 : page de dÃ©part uniquement
var emails = webCrawler.GetEmailsInPageAndChildPages(browser, "C:/TestHtml/index.html", 0);
// RÃ©sultat : ["nullepart@mozilla.org"]

// Profondeur 1 : page de dÃ©part + pages directement rÃ©fÃ©rencÃ©es
emails = webCrawler.GetEmailsInPageAndChildPages(browser, "C:/TestHtml/index.html", 1);
// RÃ©sultat : ["nullepart@mozilla.org", "ailleurs@mozilla.org"]

// Profondeur 2 : page de dÃ©part + 2 niveaux de pages rÃ©fÃ©rencÃ©es
emails = webCrawler.GetEmailsInPageAndChildPages(browser, "C:/TestHtml/index.html", 2);
// RÃ©sultat : ["nullepart@mozilla.org", "ailleurs@mozilla.org", "loin@mozilla.org"]

// Profondeur -1 : exploration complÃ¨te sans limite
emails = webCrawler.GetEmailsInPageAndChildPages(browser, "C:/TestHtml/index.html", -1);
```

## ğŸ“ Structure du Projet

```
WebCrawlerDemo/
â”œâ”€â”€ Interfaces.cs           # DÃ©finition des interfaces ITheTest et IWebBrowser
â”œâ”€â”€ EmailWebCrawler.cs      # ImplÃ©mentation de l'algorithme BFS
â”œâ”€â”€ MockWebBrowser.cs       # Navigateur fictif pour les tests
â”œâ”€â”€ Program.cs              # Point d'entrÃ©e avec exemples de tests
â””â”€â”€ README.md               # Documentation
```

## ğŸ”§ Technologies

- **.NET 8.0**
- **C# 12**
- **System.Xml.Linq** pour le parsing XML/HTML
- **SOLID Principles** : SÃ©paration des responsabilitÃ©s, injection de dÃ©pendances

## ğŸ“Š Exemple de Test

### Structure HTML de Test

**index.html**
```html
<html>
  <h1>INDEX</h1>
  <a href="./child1.html">child1</a>
  <a href="mailto:nullepart@mozilla.org">Envoyer l'email nulle part</a>
</html>
```

**child1.html**
```html
<html>
  <h1>CHILD1</h1>
  <a href="./index.html">index</a>
  <a href="./child2.html">child2</a>
  <a href="mailto:ailleurs@mozilla.org">Envoyer l'email ailleurs</a>
  <a href="mailto:nullepart@mozilla.org">Envoyer l'email nulle part</a>
</html>
```

**child2.html**
```html
<html>
  <h1>CHILD2</h1>
  <a href="./index.html">index</a>
  <a href="mailto:loin@mozilla.org">Envoyer l'email loin</a>
  <a href="mailto:nullepart@mozilla.org">Envoyer l'email nulle part</a>
</html>
```

### RÃ©sultats Attendus

```
Profondeur 0: nullepart@mozilla.org
Profondeur 1: nullepart@mozilla.org, ailleurs@mozilla.org
Profondeur 2: nullepart@mozilla.org, ailleurs@mozilla.org, loin@mozilla.org
```

## âš¡ CaractÃ©ristiques Techniques

### Gestion des Cycles
Le crawler dÃ©tecte et Ã©vite les cycles grÃ¢ce Ã  un `HashSet<string>` qui stocke les URLs dÃ©jÃ  visitÃ©es.

```csharp
var visitedUrls = new HashSet<string>();
if (visitedUrls.Contains(currentUrl))
    continue;
```

### Extraction des Emails
Les emails sont extraits des liens `mailto:` et validÃ©s avec une expression rÃ©guliÃ¨re :

```csharp
var mailtoLinks = doc.Descendants("a")
    .Where(a => a.Attribute("href")?.Value.StartsWith("mailto:") == true);
```

### RÃ©solution des URLs Relatives
Le crawler gÃ¨re correctement les URLs relatives (`./child1.html`) :

```csharp
if (relativeUrl.StartsWith("./"))
    relativeUrl = relativeUrl.Substring(2);
string absoluteUrl = Path.Combine(baseDirectory, relativeUrl);
```

### UnicitÃ© des RÃ©sultats
Un `HashSet<string>` garantit qu'aucun email n'est retournÃ© en double, mÃªme s'il apparaÃ®t sur plusieurs pages.

## ğŸ“ˆ ComplexitÃ©

- **Temps** : O(V + E)
  - V = nombre de pages visitÃ©es
  - E = nombre de liens explorÃ©s
  
- **Espace** : O(V)
  - Stockage des URLs visitÃ©es
  - Queue pour le parcours BFS

## ğŸƒ ExÃ©cution

```bash
# Cloner le repository
git clone https://github.com/[username]/WebCrawlerDemo.git

# Naviguer dans le projet
cd WebCrawlerDemo

# Restaurer les dÃ©pendances
dotnet restore

# ExÃ©cuter l'application
dotnet run
```

## ğŸ§ª Tests

Le projet inclut des tests manuels dans `Program.cs` qui valident :
- âœ… Extraction correcte des emails Ã  diffÃ©rentes profondeurs
- âœ… Gestion des emails en double (unicitÃ©)
- âœ… Gestion des cycles (index â†’ child1 â†’ index)
- âœ… Respect de la profondeur maximale
- âœ… Exploration illimitÃ©e avec `maximumDepth = -1`

## ğŸ“ Principes de Design

### SOLID Principles

- **Single Responsibility** : Chaque mÃ©thode a une responsabilitÃ© unique
- **Open/Closed** : Extensible via l'interface `IWebBrowser`
- **Liskov Substitution** : `MockWebBrowser` peut Ãªtre remplacÃ© par toute implÃ©mentation de `IWebBrowser`
- **Interface Segregation** : Interfaces ciblÃ©es et minimales
- **Dependency Inversion** : Le crawler dÃ©pend de l'abstraction `IWebBrowser`, pas d'une implÃ©mentation concrÃ¨te

### Clean Code

- Nommage expressif des variables et mÃ©thodes
- MÃ©thodes courtes et focalisÃ©es
- Documentation XML pour toutes les mÃ©thodes publiques
- Gestion appropriÃ©e des erreurs

## ğŸ“ Notes Techniques

- Le HTML est traitÃ© comme du XML valide (hypothÃ¨se de l'exercice)
- Les emails sont normalisÃ©s en minuscules pour l'unicitÃ©
- Les URLs absolues HTTP/HTTPS sont ignorÃ©es (focus sur les liens locaux)
- Les paramÃ¨tres dans les liens `mailto:` (ex: `?subject=...`) sont automatiquement retirÃ©s

## ğŸ¤ Contribution

Ce projet est un exercice algorithmique dÃ©montrant l'implÃ©mentation d'un web crawler avec BFS. Les contributions sont bienvenues pour :
- AmÃ©liorer la robustesse du parsing HTML
- Ajouter des tests unitaires (xUnit)
- Optimiser les performances
- Ã‰tendre les fonctionnalitÃ©s

## ğŸ“„ Licence

Ce projet est fourni Ã  des fins Ã©ducatives et de dÃ©monstration.

## ğŸ‘¨â€ğŸ’» Auteur

DÃ©veloppÃ© comme dÃ©monstration d'algorithme de web crawling avec contrÃ´le de profondeur.

---

**Note** : Ce projet se concentre sur l'algorithme de crawling. Pour une utilisation en production, il faudrait ajouter :
- Gestion du HTML rÃ©el (pas uniquement XML valide)
- Support des URLs absolues et domaines multiples
- Rate limiting et politeness policies
- Gestion du robots.txt
- Tests unitaires complets
- Logging structurÃ©

Test : dotnet run
sortie:
=== Test du Web Crawler d'Emails ===

Profondeur 0:
RÃ©sultat: nullepart@mozilla.org

Profondeur 1:
RÃ©sultat: nullepart@mozilla.org, ailleurs@mozilla.org

Profondeur 2:
RÃ©sultat: nullepart@mozilla.org, ailleurs@mozilla.org, loin@mozilla.org
