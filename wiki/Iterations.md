# ItÃ©rations du Projet WebCrawlerDemo

Ce document dÃ©taille chaque itÃ©ration du projet, les fonctionnalitÃ©s ajoutÃ©es, les dÃ©fis rencontrÃ©s et les solutions apportÃ©es.

---

## âœ… ItÃ©ration 1 : Algorithme BFS de Base avec HTML/XML Valide

**Date** : 29 octobre 2025  
**Commit** : `893543b` - "Initial commit: Email Web Crawler with BFS algorithm"  
**Statut** : âœ… ComplÃ©tÃ©

### ğŸ¯ Objectifs

- ImplÃ©menter l'algorithme BFS (Breadth-First Search) pour le crawling
- Extraire les emails des liens `mailto:` dans les pages HTML
- GÃ©rer la profondeur de recherche configurable
- Ã‰viter les cycles (pages qui se rÃ©fÃ©rencent mutuellement)
- Garantir l'unicitÃ© des emails retournÃ©s

### ğŸ“‹ FonctionnalitÃ©s ImplÃ©mentÃ©es

#### 1. **Algorithme BFS**
```csharp
var urlsToVisit = new Queue<(string Url, int Depth)>();
while (urlsToVisit.Count > 0)
{
    var (currentUrl, currentDepth) = urlsToVisit.Dequeue();
    // Traitement niveau par niveau
}
```

**Avantages du BFS** :
- Explore tous les liens Ã  profondeur `n` avant de passer Ã  `n+1`
- Respecte la contrainte de prioritÃ© aux pages "les plus proches"
- ContrÃ´le naturel de la profondeur maximale

#### 2. **Extraction des Emails**
```csharp
var mailtoLinks = doc.Descendants("a")
    .Where(a => a.Attribute("href")?.Value.StartsWith("mailto:") == true);
```

- Extraction via parsing XML (XDocument)
- Validation avec regex
- Normalisation en minuscules pour l'unicitÃ©

#### 3. **Gestion des Cycles**
```csharp
var visitedUrls = new HashSet<string>();
if (visitedUrls.Contains(currentUrl))
    continue;
```

- HashSet pour tracker les URLs visitÃ©es
- Ã‰vite les boucles infinies (index â†’ child1 â†’ index)

#### 4. **ContrÃ´le de Profondeur**
```csharp
if (maximumDepth >= 0 && currentDepth > maximumDepth)
    continue;
```

- Profondeur configurable (0, 1, 2, ...)
- Support de `-1` pour exploration illimitÃ©e

### ğŸ—ï¸ Architecture

**Classes principales** :
- `ITheTest` - Interface du crawler
- `IWebBrowser` - Interface du navigateur (abstraction)
- `EmailWebCrawler` - ImplÃ©mentation BFS
- `MockWebBrowser` - Navigateur de test

**Patterns utilisÃ©s** :
- âœ… **Dependency Injection** - `IWebBrowser` injectÃ©
- âœ… **Single Responsibility** - MÃ©thodes dÃ©diÃ©es
- âœ… **Strategy Pattern** - Algorithme BFS encapsulÃ©

### ğŸ“Š RÃ©sultats de Test

```
Profondeur 0: nullepart@mozilla.org
Profondeur 1: nullepart@mozilla.org, ailleurs@mozilla.org
Profondeur 2: nullepart@mozilla.org, ailleurs@mozilla.org, loin@mozilla.org
```

âœ… Tous les tests passent correctement

### âš ï¸ Limitations Connues

1. **HTML doit Ãªtre du XML valide** - Utilise `XDocument.Parse()`
2. **Pas de support HTTP rÃ©el** - URLs locales uniquement
3. **Pas de rate limiting** - Peut surcharger un serveur
4. **Pas de gestion robots.txt** - Ne respecte pas les rÃ¨gles de crawling
5. **Logging minimal** - Console.WriteLine uniquement

### ğŸ“ˆ MÃ©triques

- **ComplexitÃ© temporelle** : O(V + E)
- **ComplexitÃ© spatiale** : O(V)
- **Fichiers** : 5 (Interfaces, Crawler, Browser, Program, README)
- **Lignes de code** : ~400 lignes

### ğŸ“ LeÃ§ons Apprises

1. **BFS vs DFS** : BFS est clairement supÃ©rieur pour ce cas d'usage car il garantit l'exploration des pages proches en premier
2. **XML Parsing** : Assumer que HTML = XML valide est une simplification forte
3. **SOLID** : L'utilisation d'interfaces rend le code testable et extensible

---

## âœ… ItÃ©ration 2 : Gestion du HTML RÃ©el

**Date** : 29 octobre 2025  
**Commit** : `e18d968` / `815ba01` - "Iteration 2: HTML rÃ©el avec HtmlAgilityPack + Wiki documentation"  
**Statut** : âœ… ComplÃ©tÃ©

### ğŸ¯ Objectifs

- Remplacer le parsing XML par un parser HTML robuste
- GÃ©rer le HTML malformÃ© (balises non fermÃ©es, attributs sans guillemets, etc.)
- Utiliser **HtmlAgilityPack** ou **AngleSharp** pour le parsing
- Maintenir la compatibilitÃ© avec l'interface existante

### ğŸ“‹ TÃ¢ches

- [x] Ajouter le package NuGet HtmlAgilityPack
- [x] Refactorer `ExtractEmailsFromHtml()` pour utiliser HtmlAgilityPack
- [x] Refactorer `ExtractChildUrls()` pour utiliser HtmlAgilityPack
- [x] Tester avec du HTML rÃ©el malformÃ©
- [x] Mettre Ã  jour la documentation
- [x] CrÃ©er des tests pour HTML malformÃ©

### ğŸ”§ Changements Techniques PrÃ©vus

**Avant (XDocument)** :
```csharp
var doc = XDocument.Parse($"<root>{html}</root>");
var mailtoLinks = doc.Descendants("a");
```

**AprÃ¨s (HtmlAgilityPack)** :
```csharp
var doc = new HtmlDocument();
doc.LoadHtml(html);
var mailtoLinks = doc.DocumentNode.SelectNodes("//a[@href]");
```

### âš ï¸ Risques et DÃ©fis

1. **Performance** - Parser HTML peut Ãªtre plus lent que XML
2. **CompatibilitÃ©** - S'assurer que les tests existants passent
3. **DÃ©pendances** - Ajout d'une dÃ©pendance externe

### ğŸ“ˆ CritÃ¨res de SuccÃ¨s

- âœ… Tous les tests existants passent
- âœ… Peut parser du HTML avec des balises non fermÃ©es
- âœ… Peut parser du HTML avec des attributs sans guillemets
- âœ… Performance similaire ou meilleure

### ğŸ¯ RÃ©sultats Obtenus

**Changements implÃ©mentÃ©s** :

1. **Migration vers HtmlAgilityPack** :
```csharp
// Avant (XDocument)
var doc = XDocument.Parse($"<root>{html}</root>");

// AprÃ¨s (HtmlAgilityPack)
var htmlDoc = new HtmlDocument();
htmlDoc.LoadHtml(html);
```

2. **Parsing robuste avec XPath** :
```csharp
var linkNodes = doc.DocumentNode.SelectNodes("//a[@href]");
```

3. **Test avec HTML malformÃ©** :
- Balises non fermÃ©es : `<h1>HTML MALFORMÃ‰` âœ…
- Attributs sans guillemets : `href=mailto:test@test.org` âœ…
- Guillemets simples : `href='./link.html'` âœ…

**Tests validÃ©s** :
```
Profondeur 0: nullepart@mozilla.org âœ…
Profondeur 1: nullepart@mozilla.org, ailleurs@mozilla.org âœ…
Profondeur 2: nullepart@mozilla.org, ailleurs@mozilla.org, loin@mozilla.org âœ…
HTML MalformÃ©: test@malformed.org, another@test.com âœ…
```

### ğŸ“š Documentation CrÃ©Ã©e

- **Wiki complet** avec 3 pages :
  - `Home.md` - Page d'accueil du wiki
  - `Iterations.md` - Documentation dÃ©taillÃ©e des itÃ©rations
  - `IA-Accelerateur-Developpeurs-Seniors.md` - RÃ©flexion sur l'utilisation de l'IA

### ğŸ“ LeÃ§ons Apprises

1. **HtmlAgilityPack vs XDocument** : HtmlAgilityPack est beaucoup plus tolÃ©rant et adaptÃ© au HTML rÃ©el
2. **XPath** : Expressions XPath plus simples et lisibles que LINQ to XML
3. **Robustesse** : Peut maintenant gÃ©rer du HTML de sources variÃ©es (web scraping rÃ©el)

### ğŸ“¦ DÃ©pendances AjoutÃ©es

- `HtmlAgilityPack` v1.12.4

---

## ğŸ“‹ ItÃ©ration 3 : Support des URLs Absolues et Domaines Multiples

**Statut** : ğŸ“‹ PlanifiÃ©

### ğŸ¯ Objectifs

- Support des URLs HTTP/HTTPS complÃ¨tes
- Crawler sur plusieurs domaines
- RÃ©solution correcte des URLs relatives et absolues
- Respect des redirections HTTP

### ğŸ“‹ TÃ¢ches PrÃ©vues

- [ ] ImplÃ©menter un vrai `HttpWebBrowser` avec HttpClient
- [ ] GÃ©rer les URLs absolues (http://, https://)
- [ ] RÃ©soudre les URLs relatives correctement avec `Uri`
- [ ] GÃ©rer les redirections (301, 302)
- [ ] Ajouter une option pour limiter aux sous-domaines

---

## ğŸ“‹ ItÃ©ration 4 : Rate Limiting et Politeness Policies

**Statut** : ğŸ“‹ PlanifiÃ©

### ğŸ¯ Objectifs

- ImplÃ©menter un dÃ©lai entre les requÃªtes
- Limiter le nombre de requÃªtes par seconde
- Respecter les bonnes pratiques de crawling

---

## ğŸ“‹ ItÃ©ration 5 : Gestion du robots.txt

**Statut** : ğŸ“‹ PlanifiÃ©

### ğŸ¯ Objectifs

- Parser le fichier robots.txt
- Respecter les directives User-agent
- GÃ©rer les rÃ¨gles Allow/Disallow

---

## ğŸ“‹ ItÃ©ration 6 : Tests Unitaires Complets

**Statut** : ğŸ“‹ PlanifiÃ©

### ğŸ¯ Objectifs

- Tests unitaires avec xUnit
- Tests d'intÃ©gration
- Code coverage > 80%

---

## ğŸ“‹ ItÃ©ration 7 : Logging StructurÃ©

**Statut** : ğŸ“‹ PlanifiÃ©

### ğŸ¯ Objectifs

- ImplÃ©menter Serilog ou NLog
- Logs structurÃ©s (JSON)
- Niveaux de log appropriÃ©s (Debug, Info, Warning, Error)

---

## ğŸ“Š Vue d'Ensemble du ProgrÃ¨s

| ItÃ©ration | FonctionnalitÃ© | Statut | Commit |
|-----------|---------------|--------|--------|
| 1 | Algorithme BFS de base | âœ… | 893543b |
| 2 | HTML rÃ©el | âœ… | e18d968 / 815ba01 |
| 3 | URLs absolues | ğŸ“‹ | - |
| 4 | Rate limiting | ğŸ“‹ | - |
| 5 | robots.txt | ğŸ“‹ | - |
| 6 | Tests unitaires | ğŸ“‹ | - |
| 7 | Logging structurÃ© | ğŸ“‹ | - |