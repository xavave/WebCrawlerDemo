# ItÃ©rations du Projet WebCrawlerDemo

Ce document dÃ©taille chaque itÃ©ration du projet, les fonctionnalitÃ©s ajoutÃ©es, les dÃ©fis rencontrÃ©s et les solutions apportÃ©es.

---

## âœ… ItÃ©ration 1 : Algorithme BFS de Base avec HTML/XML Valide

**Date** : 29 octobre 2025  
**Commit** : `893543b` - "Initial commit: Email Web Crawler with BFS algorithm"  
**Statut** : âœ… ComplÃ©tÃ©

### ğŸ¯ Objectifs

- ImplÃ©menter l'algorithme BFS (Breadth-First Search) pour le crawling
- Extraire les emails des liens `mailto:` dans les pages HTML
- GÃ©rer la profondeur de recherche configurable
- Ã‰viter les cycles (pages qui se rÃ©fÃ©rencent mutuellement)
- Garantir l'unicitÃ© des emails retournÃ©s

### ğŸ“‹ FonctionnalitÃ©s ImplÃ©mentÃ©es

#### 1. **Algorithme BFS**
```csharp
var urlsToVisit = new Queue<(string Url, int Depth)>();
while (urlsToVisit.Count > 0)
{
    var (currentUrl, currentDepth) = urlsToVisit.Dequeue();
    // Traitement niveau par niveau
}
```

**Avantages du BFS** :
- Explore tous les liens Ã  profondeur `n` avant de passer Ã  `n+1`
- Respecte la contrainte de prioritÃ© aux pages "les plus proches"
- ContrÃ´le naturel de la profondeur maximale

#### 2. **Extraction des Emails**
```csharp
var mailtoLinks = doc.Descendants("a")
    .Where(a => a.Attribute("href")?.Value.StartsWith("mailto:") == true);
```

- Extraction via parsing XML (XDocument)
- Validation avec regex
- Normalisation en minuscules pour l'unicitÃ©

#### 3. **Gestion des Cycles**
```csharp
var visitedUrls = new HashSet<string>();
if (visitedUrls.Contains(currentUrl))
    continue;
```

- HashSet pour tracker les URLs visitÃ©es
- Ã‰vite les boucles infinies (index â†’ child1 â†’ index)

#### 4. **ContrÃ´le de Profondeur**
```csharp
if (maximumDepth >= 0 && currentDepth > maximumDepth)
    continue;
```

- Profondeur configurable (0, 1, 2, ...)
- Support de `-1` pour exploration illimitÃ©e

### ğŸ—ï¸ Architecture

**Classes principales** :
- `ITheTest` - Interface du crawler
- `IWebBrowser` - Interface du navigateur (abstraction)
- `EmailWebCrawler` - ImplÃ©mentation BFS
- `MockWebBrowser` - Navigateur de test

**Patterns utilisÃ©s** :
- âœ… **Dependency Injection** - `IWebBrowser` injectÃ©
- âœ… **Single Responsibility** - MÃ©thodes dÃ©diÃ©es
- âœ… **Strategy Pattern** - Algorithme BFS encapsulÃ©

### ğŸ“Š RÃ©sultats de Test

```
Profondeur 0: nullepart@mozilla.org
Profondeur 1: nullepart@mozilla.org, ailleurs@mozilla.org
Profondeur 2: nullepart@mozilla.org, ailleurs@mozilla.org, loin@mozilla.org
```

âœ… Tous les tests passent correctement

### âš ï¸ Limitations Connues

1. **HTML doit Ãªtre du XML valide** - Utilise `XDocument.Parse()`
2. **Pas de support HTTP rÃ©el** - URLs locales uniquement
3. **Pas de rate limiting** - Peut surcharger un serveur
4. **Pas de gestion robots.txt** - Ne respecte pas les rÃ¨gles de crawling
5. **Logging minimal** - Console.WriteLine uniquement

### ğŸ“ˆ MÃ©triques

- **ComplexitÃ© temporelle** : O(V + E)
- **ComplexitÃ© spatiale** : O(V)
- **Fichiers** : 5 (Interfaces, Crawler, Browser, Program, README)
- **Lignes de code** : ~400 lignes

### ğŸ“ LeÃ§ons Apprises

1. **BFS vs DFS** : BFS est clairement supÃ©rieur pour ce cas d'usage car il garantit l'exploration des pages proches en premier
2. **XML Parsing** : Assumer que HTML = XML valide est une simplification forte
3. **SOLID** : L'utilisation d'interfaces rend le code testable et extensible

---

## âœ… ItÃ©ration 2 : Gestion du HTML RÃ©el

**Date** : 29 octobre 2025  
**Commit** : `e18d968` / `815ba01` - "Iteration 2: HTML rÃ©el avec HtmlAgilityPack + Wiki documentation"  
**Statut** : âœ… ComplÃ©tÃ©

### ğŸ¯ Objectifs

- Remplacer le parsing XML par un parser HTML robuste
- GÃ©rer le HTML malformÃ© (balises non fermÃ©es, attributs sans guillemets, etc.)
- Utiliser **HtmlAgilityPack** ou **AngleSharp** pour le parsing
- Maintenir la compatibilitÃ© avec l'interface existante

### ğŸ“‹ TÃ¢ches

- [x] Ajouter le package NuGet HtmlAgilityPack
- [x] Refactorer `ExtractEmailsFromHtml()` pour utiliser HtmlAgilityPack
- [x] Refactorer `ExtractChildUrls()` pour utiliser HtmlAgilityPack
- [x] Tester avec du HTML rÃ©el malformÃ©
- [x] Mettre Ã  jour la documentation
- [x] CrÃ©er des tests pour HTML malformÃ©

### ğŸ”§ Changements Techniques PrÃ©vus

**Avant (XDocument)** :
```csharp
var doc = XDocument.Parse($"<root>{html}</root>");
var mailtoLinks = doc.Descendants("a");
```

**AprÃ¨s (HtmlAgilityPack)** :
```csharp
var doc = new HtmlDocument();
doc.LoadHtml(html);
var mailtoLinks = doc.DocumentNode.SelectNodes("//a[@href]");
```

### âš ï¸ Risques et DÃ©fis

1. **Performance** - Parser HTML peut Ãªtre plus lent que XML
2. **CompatibilitÃ©** - S'assurer que les tests existants passent
3. **DÃ©pendances** - Ajout d'une dÃ©pendance externe

### ğŸ“ˆ CritÃ¨res de SuccÃ¨s

- âœ… Tous les tests existants passent
- âœ… Peut parser du HTML avec des balises non fermÃ©es
- âœ… Peut parser du HTML avec des attributs sans guillemets
- âœ… Performance similaire ou meilleure

### ğŸ¯ RÃ©sultats Obtenus

**Changements implÃ©mentÃ©s** :

1. **Migration vers HtmlAgilityPack** :
```csharp
// Avant (XDocument)
var doc = XDocument.Parse($"<root>{html}</root>");

// AprÃ¨s (HtmlAgilityPack)
var htmlDoc = new HtmlDocument();
htmlDoc.LoadHtml(html);
```

2. **Parsing robuste avec XPath** :
```csharp
var linkNodes = doc.DocumentNode.SelectNodes("//a[@href]");
```

3. **Test avec HTML malformÃ©** :
- Balises non fermÃ©es : `<h1>HTML MALFORMÃ‰` âœ…
- Attributs sans guillemets : `href=mailto:test@test.org` âœ…
- Guillemets simples : `href='./link.html'` âœ…

**Tests validÃ©s** :
```
Profondeur 0: nullepart@mozilla.org âœ…
Profondeur 1: nullepart@mozilla.org, ailleurs@mozilla.org âœ…
Profondeur 2: nullepart@mozilla.org, ailleurs@mozilla.org, loin@mozilla.org âœ…
HTML MalformÃ©: test@malformed.org, another@test.com âœ…
```

### ğŸ“š Documentation CrÃ©Ã©e

- **Wiki complet** avec 3 pages :
  - `Home.md` - Page d'accueil du wiki
  - `Iterations.md` - Documentation dÃ©taillÃ©e des itÃ©rations
  - `IA-Accelerateur-Developpeurs-Seniors.md` - RÃ©flexion sur l'utilisation de l'IA

### ğŸ“ LeÃ§ons Apprises

1. **HtmlAgilityPack vs XDocument** : HtmlAgilityPack est beaucoup plus tolÃ©rant et adaptÃ© au HTML rÃ©el
2. **XPath** : Expressions XPath plus simples et lisibles que LINQ to XML
3. **Robustesse** : Peut maintenant gÃ©rer du HTML de sources variÃ©es (web scraping rÃ©el)

### ğŸ“¦ DÃ©pendances AjoutÃ©es

- `HtmlAgilityPack` v1.12.4

---

## âœ… ItÃ©ration 3 : Support des URLs HTTP/HTTPS et Domaines Multiples

**Date** : 29 octobre 2025  
**Commit** : `1c787fe` - "Iteration 3: HTTP/HTTPS URLs support"  
**Statut** : âœ… ComplÃ©tÃ©

### ğŸ¯ Objectifs

- Support des URLs HTTP/HTTPS complÃ¨tes
- Crawler sur plusieurs domaines
- RÃ©solution correcte des URLs relatives et absolues avec `Uri`
- Respect des redirections HTTP
- Normalisation avancÃ©e des URLs

### ğŸ“‹ FonctionnalitÃ©s ImplÃ©mentÃ©es

#### 1. **HttpWebBrowser avec HttpClient**
```csharp
public class HttpWebBrowser : IWebBrowser
{
    private readonly HttpClient _httpClient;
    
    public string? GetHtml(string url)
    {
        var response = _httpClient.GetAsync(url).Result;
        return response.Content.ReadAsStringAsync().Result;
    }
}
```

**CaractÃ©ristiques** :
- HttpClient avec redirections automatiques (max 5)
- Timeout configurable (30 secondes)
- User-Agent personnalisable
- Gestion des erreurs HTTP

#### 2. **RÃ©solution d'URLs avec Uri**
```csharp
private string ResolveRelativeUrl(string baseUrl, string relativeUrl)
{
    var baseUri = new Uri(baseUrl);
    var resolvedUri = new Uri(baseUri, relativeUrl);
    return resolvedUri.ToString();
}
```

**Support** :
- URLs relatives : `./child.html`, `../parent.html`, `child.html`
- URLs absolues : `https://example.com/page.html`
- Chemins locaux : `C:/TestHtml/index.html`

#### 3. **Normalisation des URLs**
```csharp
private string NormalizeUrl(string url)
{
    // Lowercase scheme et host
    // Suppression default ports (80, 443)
    // Suppression fragments (#section)
    // Normalisation path
}
```

### ğŸ“Š RÃ©sultats de Test

âœ… Support HTTP/HTTPS complet
âœ… RÃ©solution d'URLs relatives correcte
âœ… Normalisation avancÃ©e (ports, fragments)
âœ… Tous les tests prÃ©cÃ©dents passent

### ğŸ“ LeÃ§ons Apprises

1. **Uri classe** : GÃ¨re automatiquement la rÃ©solution relative/absolue
2. **HttpClient** : Doit Ãªtre rÃ©utilisÃ© (singleton pattern recommandÃ©)
3. **Normalisation** : Critique pour Ã©viter les doublons d'URLs

---

## âœ… ItÃ©ration 4 : Rate Limiting et Politeness Policies

**Date** : 29 octobre 2025  
**Commit** : `bc56821` - "Iteration 4: Rate limiting and politeness policies"  
**Statut** : âœ… ComplÃ©tÃ©

### ğŸ¯ Objectifs

- ImplÃ©menter un dÃ©lai entre les requÃªtes par domaine
- Limiter le nombre de pages par domaine
- CrÃ©er des politiques de crawling configurables
- Respecter les bonnes pratiques de crawling Ã©thique

### ğŸ“‹ FonctionnalitÃ©s ImplÃ©mentÃ©es

#### 1. **Rate Limiting par Domaine**
```csharp
private void ApplyRateLimiting(string url)
{
    var domain = ExtractDomain(url);
    var timeSinceLastRequest = DateTime.UtcNow - _lastRequestPerDomain[domain];
    var remainingDelay = _delayBetweenRequestsMs - timeSinceLastRequest.TotalMilliseconds;
    
    if (remainingDelay > 0)
        Thread.Sleep((int)remainingDelay);
}
```

**Thread-safe** avec `lock()` pour accÃ¨s concurrent

#### 2. **Limitation de Pages par Domaine**
```csharp
private bool CanCrawlDomain(string url)
{
    if (_policies.MaxPagesPerDomain < 0)
        return true; // IllimitÃ©
        
    var domain = ExtractDomain(url);
    return _pagesPerDomain[domain] < _policies.MaxPagesPerDomain;
}
```

#### 3. **CrawlerPolicies Configurables**
```csharp
public class CrawlerPolicies
{
    public int DelayBetweenRequestsMs { get; set; } = 1000;
    public int MaxPagesPerDomain { get; set; } = -1;
    public int RequestTimeoutSeconds { get; set; } = 30;
    public string UserAgent { get; set; } = "WebCrawlerDemo/1.0 (+GitHub)";
    
    public static CrawlerPolicies Default { get; }      // Ã‰quilibrÃ©
    public static CrawlerPolicies Conservative { get; } // Respectueux
    public static CrawlerPolicies Aggressive { get; }   // Tests
}
```

### ğŸ“Š Politiques PrÃ©dÃ©finies

| Politique | DÃ©lai (ms) | Max Pages | Timeout (s) | Usage |
|-----------|-----------|-----------|-------------|-------|
| Default | 1000 | IllimitÃ© | 30 | Production standard |
| Conservative | 2000 | 100 | 20 | Sites sensibles |
| Aggressive | 100 | IllimitÃ© | 10 | Tests uniquement |

### ğŸ“ LeÃ§ons Apprises

1. **Rate limiting** : Essentiel pour ne pas surcharger les serveurs
2. **Thread-safety** : Lock nÃ©cessaire pour le dictionnaire de timestamps
3. **Configuration** : Politiques prÃ©dÃ©finies facilitent l'utilisation

---

## âœ… ItÃ©ration 5 : Gestion du robots.txt

**Date** : 29 octobre 2025  
**Commit** : `018f50c` - "Iteration 5: robots.txt support"  
**Statut** : âœ… ComplÃ©tÃ©

### ğŸ¯ Objectifs

- Parser le fichier robots.txt selon la RFC
- Respecter les directives User-agent, Disallow, Allow
- GÃ©rer Crawl-delay et Sitemap
- Cache thread-safe par domaine

### ğŸ“‹ FonctionnalitÃ©s ImplÃ©mentÃ©es

#### 1. **RobotsTxtParser RFC-compliant**
```csharp
public class RobotsTxtParser
{
    public void Parse(string robotsTxtContent);
    public bool IsAllowed(string url, string userAgent);
    public int? GetCrawlDelay();
    public List<string> GetSitemaps();
}
```

**Directives supportÃ©es** :
- `User-agent:` - Ciblage par crawler
- `Disallow:` - Chemins interdits
- `Allow:` - Chemins autorisÃ©s (prioritÃ© sur Disallow)
- `Crawl-delay:` - DÃ©lai minimum entre requÃªtes
- `Sitemap:` - URLs des sitemaps
- Commentaires (`#`)
- Wildcards (`*` et `$`)

#### 2. **RobotsTxtCache Thread-Safe**
```csharp
public class RobotsTxtCache
{
    public RobotsTxtParser GetRobotsTxt(string url);
    public bool IsAllowed(string url, string userAgent);
}
```

**CaractÃ©ristiques** :
- Cache par domaine avec expiration (24h)
- Thread-safe avec `lock()`
- TÃ©lÃ©chargement automatique de `/robots.txt`

#### 3. **IntÃ©gration au Crawler**
```csharp
if (_policies.RespectRobotsTxt && !_robotsCache.IsAllowed(url, userAgent))
{
    Log.Information("URL bloquÃ©e par robots.txt: {Url}", url);
    continue;
}
```

### ğŸ“Š RÃ©sultats de Test

âœ… Parser complet fonctionnel
âœ… RÃ¨gles Allow/Disallow respectÃ©es
âœ… Crawl-delay et Sitemap extraits
âœ… Cache par domaine opÃ©rationnel
âœ… Multi-user-agents gÃ©rÃ© correctement

### ğŸ“ LeÃ§ons Apprises

1. **RFC robots.txt** : RÃ¨gles complexes avec prioritÃ©s (plus spÃ©cifique gagne)
2. **Cache** : Ã‰vite de tÃ©lÃ©charger robots.txt Ã  chaque requÃªte
3. **Ã‰thique** : Respecter robots.txt est fondamental pour un crawler responsable

---

## âœ… ItÃ©ration 6 : Tests Unitaires Complets (xUnit)

**Date** : 29 octobre 2025  
**Commits** : `b4a22b3`, `f851efa` - "Iteration 6: xUnit tests + fixes"  
**Statut** : âœ… ComplÃ©tÃ©

### ğŸ¯ Objectifs

- Tests unitaires avec xUnit
- Coverage de tous les composants principaux
- Tests des edge cases
- 100% de rÃ©ussite des tests

### ğŸ“‹ Tests ImplÃ©mentÃ©s

#### **EmailWebCrawlerTests.cs** (10 tests)
- âœ… Depth 0/1/2 - Emails extraits corrects
- âœ… HTML malformÃ© - Parser robuste
- âœ… Emails distincts - UnicitÃ© garantie
- âœ… Normalisation lowercase
- âœ… MaxPagesPerDomain - Limite respectÃ©e
- âœ… Validation paramÃ¨tres (null, empty)

#### **RobotsTxtParserTests.cs** (10 tests)
- âœ… Empty content - Tout autorisÃ©
- âœ… Disallow all - Tout bloquÃ©
- âœ… Allow/Disallow - RÃ¨gle spÃ©cifique gagne
- âœ… Crawl-delay - Valeur extraite
- âœ… Sitemap - URLs extraites
- âœ… Commentaires - IgnorÃ©s (ligne + inline)
- âœ… Wildcards - Patterns * et $
- âœ… Multi-user-agents - PrioritÃ© correcte

#### **CrawlerPoliciesTests.cs** (3 tests)
- âœ… Default policy - Valeurs Ã©quilibrÃ©es
- âœ… Conservative policy - Valeurs respectueuses
- âœ… Aggressive policy - Valeurs rapides

### ğŸ“Š RÃ©sultats

```
SÃ©rie de tests rÃ©ussie.
Nombre total de tests : 23
     RÃ©ussi(s) : 23
 DurÃ©e totale : 0,49 secondes
```

### ğŸ› Bugs CorrigÃ©s

1. **Commentaires inline** - Parser ne gÃ©rait pas `# comment` aprÃ¨s directive
2. **Multi-user-agents** - RÃ¨gles wildcard pas priorisÃ©es correctement
3. **Domain limiting** - Chemins locaux non dÃ©tectÃ©s comme domaine

### ğŸ“ LeÃ§ons ApprÃ©es

1. **TDD** : Tests rÃ©vÃ¨lent bugs subtils dans la logique
2. **Arrange-Act-Assert** : Pattern clair et maintenable
3. **Edge cases** : Tests de validation essentiels

---

## âœ… ItÃ©ration 7 : Structured Logging avec Serilog

**Date** : 29 octobre 2025  
**Commit** : `2f513cf` - "Iteration 7: Structured logging with Serilog"  
**Statut** : âœ… ComplÃ©tÃ©

### ğŸ¯ Objectifs

- ImplÃ©menter Serilog pour logging structurÃ©
- Console et File sinks
- Niveaux de log appropriÃ©s (Debug, Info, Warning, Error)
- Logs JSON structurÃ©s dans fichiers
- Rotation quotidienne des logs

### ğŸ“‹ FonctionnalitÃ©s ImplÃ©mentÃ©es

#### 1. **LoggingConfiguration CentralisÃ©**
```csharp
public static class LoggingConfiguration
{
    public static void ConfigureLogging(LogEventLevel minimumLevel = LogEventLevel.Information)
    {
        Log.Logger = new LoggerConfiguration()
            .MinimumLevel.Is(minimumLevel)
            .WriteTo.Console(outputTemplate: "[{Timestamp:HH:mm:ss} {Level:u3}] {Message:lj}")
            .WriteTo.File(
                path: "logs/webcrawler-.log",
                rollingInterval: RollingInterval.Day,
                retainedFileCountLimit: 7)
            .CreateLogger();
    }
}
```

#### 2. **Logging dans Toutes les Classes**

**EmailWebCrawler** :
```csharp
Log.Information("DÃ©but du crawling - URL: {Url}, Profondeur max: {MaxDepth}", url, maximumDepth);
Log.Debug("Emails trouvÃ©s - URL: {Url}, Nombre: {Count}, Emails: {@Emails}", url, count, emails);
Log.Warning("Limite de pages atteinte - URL: {Url}, Domaine: {Domain}", url, domain);
Log.Error(ex, "Erreur lors du traitement de l'URL: {Url}", url);
```

**HttpWebBrowser** :
```csharp
Log.Debug("RÃ©cupÃ©ration de la page - URL: {Url}", url);
Log.Debug("Rate limiting - Domaine: {Domain}, Attente: {Delay}ms", domain, delay);
```

**RobotsTxtParser** :
```csharp
Log.Debug("Parsing robots.txt - Taille: {Size} caractÃ¨res", size);
```

#### 3. **Niveaux de Log UtilisÃ©s**

| Niveau | Usage | Exemples |
|--------|-------|----------|
| **Debug** | DÃ©tails techniques | URL normalisÃ©e, liens enfants, cache |
| **Information** | Ã‰vÃ©nements importants | DÃ©but/fin crawling, rÃ©sultats |
| **Warning** | Situations anormales | Limite atteinte, HTML vide |
| **Error** | Erreurs avec exceptions | Erreurs HTTP, parsing |

### ğŸ“Š RÃ©sultats

**Console Output** :
```
[21:10:07 INF] DÃ©but du crawling - URL: C:/TestHtml/index.html
[21:10:07 DBG] Emails trouvÃ©s - Nombre: 1
[21:10:07 WRN] Limite de pages atteinte - Domaine: c:/testhtml
[21:10:07 INF] Crawling terminÃ© - Emails: 3, Pages: 3
```

**Fichier logs/webcrawler-20251029.log** :
```json
[2025-10-29 21:10:07.514 +01:00 INF] DÃ©but du crawling - URL: "C:/TestHtml/index.html" {"Application":"WebCrawlerDemo"}
[2025-10-29 21:10:07.626 +01:00 DBG] Emails trouvÃ©s - Nombre: 1, Emails: ["nullepart@mozilla.org"] {"Application":"WebCrawlerDemo"}
```

### ğŸ“ˆ CaractÃ©ristiques

- âœ… **Rotation quotidienne** - Nouveau fichier chaque jour
- âœ… **RÃ©tention 7 jours** - Nettoyage automatique
- âœ… **Logs structurÃ©s JSON** - Facilite parsing et analyse
- âœ… **MÃ©tadonnÃ©es** - Application, timestamp, timezone
- âœ… **ObservabilitÃ© complÃ¨te** - TraÃ§abilitÃ© de toutes les opÃ©rations

### ğŸ“ LeÃ§ons Apprises

1. **Serilog** : Flexible et puissant pour logging structurÃ©
2. **Niveaux appropriÃ©s** : Debug pour dÃ©tails, Info pour Ã©vÃ©nements, Warning pour anomalies
3. **JSON structurÃ©** : Facilite analyse avec outils (ELK, Splunk, etc.)
4. **Production-ready** : Rotation et rÃ©tention essentielles

---

## ğŸ“Š Vue d'Ensemble du ProgrÃ¨s

| ItÃ©ration | FonctionnalitÃ© | Statut | Commit |
|-----------|---------------|--------|--------|
| 1 | Algorithme BFS de base | âœ… | 893543b |
| 2 | HTML rÃ©el (HtmlAgilityPack) | âœ… | e18d968 |
| 3 | URLs HTTP/HTTPS | âœ… | 1c787fe |
| 4 | Rate limiting et politiques | âœ… | bc56821 |
| 5 | Support robots.txt | âœ… | 018f50c |
| 6 | Tests unitaires xUnit (23 tests) | âœ… | b4a22b3 |
| 7 | Logging structurÃ© (Serilog) | âœ… | 2f513cf |

## ğŸ‰ Projet Complet !

Le WebCrawlerDemo est maintenant un **crawler de niveau production** avec :
- ğŸ›¡ï¸ SÃ©curitÃ© et Ã©thique (robots.txt, rate limiting)
- ï¿½ ObservabilitÃ© (logging structurÃ©)
- âœ… QualitÃ© (23 tests unitaires, SOLID)
- âš¡ Performance (cache, normalisation)
- ğŸ¯ Robustesse (HTML malformÃ©, erreurs)
- ğŸ”§ Configuration (politiques personnalisables)